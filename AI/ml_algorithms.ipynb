{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_algorithms.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQMLbM0iNZSk"
      },
      "source": [
        "## Recommended Links :\n",
        "  \n",
        "* [python-course.eu](https://www.python-course.eu)\n",
        "* [Google Devlopers](https://developers.google.com/machine-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khjvEjEqm7Rj"
      },
      "source": [
        "# Boosting & bagging\n",
        "\n",
        "Similarities & Differences\n",
        "\n",
        "* Both are ensemble methods to get N learners from 1 learner…\n",
        "\n",
        "… but, while they are built independently for Bagging, Boosting tries to add new models that do well where previous models fail.\n",
        "\n",
        "* Both generate several training data sets by random sampling…\n",
        "\n",
        "* … but only Boosting determines weights for the data to tip the scales in favor of the most difficult cases.\n",
        "\n",
        "* Both make the final decision by averaging  the N learners (or taking the majority of them)…\n",
        "\n",
        "* … but it is an equally weighted average for Bagging and a weighted average for Boosting, more weight to those with better performance on training data.\n",
        "\n",
        "* Both are good at reducing variance and provide higher stability…\n",
        "\n",
        "* … but only Boosting tries to reduce bias. On the other hand, Bagging may solve the over-fitting problem, while Boosting can increase it.\n",
        "\n",
        "### References:\n",
        "\n",
        "* [Quantdare](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LczWAdUfnMZM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff2HU-kVGJS6"
      },
      "source": [
        "# Decesion Trees\n",
        "\n",
        "The main idea of decision trees is to find those descriptive features which contain the most \"information\" regarding the target feature and then split the dataset along the values of these features such that the target feature values for the resulting sub_datasets are as pure as possible\n",
        "\n",
        "\n",
        "## Advantages:\n",
        "\n",
        "* Applicable for categorical data, Not effected by outliers/Redundant features, No Need to impute missing data - Require very little data preparation, Interpretable, Decision Trees that is applicable for continuous and categorical inputs.\n",
        "* In Non Parametric methods no such feature of distribution is used for modelling. Primarily in Decision trees (say CART) it takes into account which variable/split brings in maximum difference in the two branches(eg - Gini) . In such a case , the distribution does not really matter.\n",
        "\n",
        "## Disadvantages:\n",
        "\n",
        " Overfitting, Follows Greedy Approach, Unstable, is affected by class imbalance.\n",
        "\n",
        "\n",
        "* **Smooth boundaries are problematic**  i.e they work best when you have discontinuous piece wise constant model. If you truly have **a linear target function decision trees are not the best.**\n",
        "\n",
        "* **Un-correlated variables is an issue**  Decision tree's work by finding the interactions between variables.  if you have a situation where there are no interactions between variables linear approaches might be the best.\n",
        "\n",
        "* **Data fragmentation** : Each split in a tree leads to a reduced dataset under consideration. And, hence the model created at the split will potentially introduce bias.\n",
        "\n",
        "* **High variance and unstable** :  As a result of the greedy strategy applied by decision tree's variance in finding the right starting point of the tree can greatly impact the final result. i.e small changes early on can have big impacts later. So- if for example you draw two different samples from your universe , the starting points for both the samples could be very different (and may even be different variables) this can lead to totally different results.\n",
        "\n",
        "* **Instability** - The decision tree changes when I perturb the dataset a bit. This is not desirable as we want our classification algorithm to be pretty robust to noise and be able to generalize well to future observed data. This can undercut confidence in the tree and hurt the ability to learn from it. One solution - Is to switch to a tree-ensemble method that combines many decision trees on slightly different versions of the dataset.\n",
        "\n",
        "* **Classification Plateaus** - There's a very big difference between being on the left side of a boundary instead of a right side. We could see two different flowers with similar characteristics classified very differently. Some sort of rolling hill type of classification could work better than a plateau classification scheme. One solution - (like above), is to switch to a tree-ensemble method that combines many decision trees on slightly different versions of the dataset.\n",
        "\n",
        "* **Decision Boundaries are parallel to the axis** - We could imagine diagonal decision boundaries that would perform better, e.g. separating the setosa flowers and the versicolor flowers.\n",
        "\n",
        "* **Interpretation**:  Decision Trees will never give you the right answer, it will give you many possible answers.  Change the root node of the tree to start with a different variable and you will probably get a different tree. Which one is correct?\n",
        "\n",
        "* **Significance**:  Change the significance level (even a little), or the size of the bins and you will get different trees. Change the random seed and it changes as well.  Repeat the process for another sample (or even subsample) and you will get another tree. Do enough permutations on your own, and you will get the tree you want!\n",
        "\n",
        "* **Multiple comparisons**: Tree algorithms generally use chi-square test to determine significance of any particular split.  But even before that has been determined, the algorithm has tried many combinations of variables to get the 'best' split.  That, coupled with a tree with many levels and branches can lead to an over optimized result, or many false positives due to multiple comparisons.\n",
        "\n",
        "### References:\n",
        "\n",
        "* [Quora](https://www.quora.com/What-are-the-disadvantages-of-using-a-decision-tree-for-classification)\n",
        "* [Research gate](researchgate.net/post/What_are_pros_and_cons_of_decision_tree_versus_other_classifier_as_KNN_SVM_NN)\n",
        "* [python-course.eu](https://www.python-course.eu/Decision_Trees.php)\n",
        "* [Visualization kdnuggets](https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya4aiD2yRKWZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6d86db99-b9d0-4b06-e0c4-aa7fe2a23031"
      },
      "source": [
        "# Decesion Tree impl\n",
        "import pandas as pd\n",
        "import graphviz\n",
        "from sklearn import tree\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Sample data\n",
        "def EDA():\n",
        "  data = pd.DataFrame({\"toothed\":[\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\"],\n",
        "                      \"hair\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"False\"],\n",
        "                      \"breathes\":[\"True\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\"],\n",
        "                      \"legs\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"True\"],\n",
        "                      \"species\":[\"Mammal\",\"Mammal\",\"Reptile\",\"Mammal\",\"Mammal\",\"Mammal\",\"Reptile\",\"Reptile\",\"Mammal\",\"Reptile\"]}, \n",
        "                      columns=[\"toothed\",\"hair\",\"breathes\",\"legs\",\"species\"])\n",
        "  features = data[[\"toothed\",\"hair\",\"breathes\",\"legs\"]]\n",
        "  target = data[\"species\"]\n",
        "  enc=LabelEncoder()\n",
        "\n",
        "  for i in data.columns:\n",
        "    data[i]=enc.fit_transform(data[i])\n",
        "  return data\n",
        "\n",
        "def  split_data(dataset):\n",
        "  X= dataset.iloc[:,:-1]\n",
        "  Y= dataset.iloc[:,-1]\n",
        "\n",
        "  from sklearn.model_selection import  train_test_split\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "def udfDtree(x_train, x_test, y_train, y_test):\n",
        "  tree_model=tree.DecisionTreeClassifier()\n",
        "  tree_model.fit(data.iloc[:-3,:-1],data.iloc[:-3,-1])\n",
        "  tree.plot_tree(tree_model)  \n",
        "\n",
        "  tree_vis=tree.export_graphviz(tree_model,out_file=None,feature_names=data.columns[:-1],class_names=data.columns[-1])\n",
        "  graph=graphviz.Source(tree_vis)\n",
        "  graph.render(\"data\")\n",
        "\n",
        "  #$tree_model.score()\n",
        "\n",
        "\n",
        "\n",
        "def sklearnDtree(x_train, x_test, y_train, y_test):\n",
        "  from  sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "  tree_model=DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, \\\n",
        "                         min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \\\n",
        "                         max_leaf_nodes=None,  min_impurity_split=None, \\\n",
        "                         class_weight=None, ccp_alpha=0.0) \n",
        "  tree_model.fit(x_train,y_train)\n",
        "  y_predicted=tree_model.predict(x_test)\n",
        "  print(tree_model.score(x_train,y_train)\n",
        "  print(tree_model.score(x_test,y_test))\n",
        "  return y_predicted\n",
        "\n",
        "data=EDA()\n",
        "\n",
        "x_train, x_test, y_train, y_test=split_data(data)\n",
        "sklearnDtree(x_train, x_test, y_train, y_test)\n",
        "udfDtree(x_train, x_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.6666666666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k6rWR5UqvZw"
      },
      "source": [
        "# KMEANS\n",
        "\n",
        "T(C) = O(no of data points X number of clusters X Dimension od data points X Number of Iterations)\n",
        "\n",
        "k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.\n",
        "\n",
        "The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the **expectation-maximization algorithm for mixtures of Gaussian distributions** via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\n",
        "\n",
        "The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\n",
        "\n",
        "The term \"k-means\" was first used by **James MacQueen in 1967**,though the idea goes back to Hugo Steinhaus in 1956.The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, though it wasn't published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as **Lloyd-Forgy**.\n",
        "\n",
        "k-means clustering, and its associated expectation-maximization algorithm, is a special case of a **Gaussian mixture model**, specifically, the limit of taking all covariances as diagonal, equal and small. It is often easy to generalize a k-means problem into a Gaussian mixture model. Another generalization of the k-means algorithm is the **K-SVD algorithm**, which estimates data points as a sparse linear combination of \"codebook vectors\". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.\n",
        "\n",
        "**Assumptions**\n",
        "* A key limitation of k-means is its cluster model. \n",
        "* The concept is based on **spherical clusters** that are separable so that the mean converges towards the cluster center.\n",
        "* The clusters are expected to be of **similar size**, so that the assignment to the nearest cluster center is the correct assignment.\n",
        "\n",
        "**Applications**\n",
        "* k-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd's algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.\n",
        "\n",
        "* Vector quantization :\n",
        "k-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.\n",
        "\n",
        "* Cluster analysis :In cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters).\n",
        "\n",
        "**Advantages** \n",
        "* Relatively simple to implement.\n",
        "\n",
        "* Scales to large data sets.\n",
        "\n",
        "* Guarantees convergence.\n",
        "\n",
        "* Can warm-start the positions of centroids.\n",
        "\n",
        "* Easily adapts to new examples.\n",
        "\n",
        "* Generalizes to clusters of different shapes and sizes, such as elliptical clusters.\n",
        "\n",
        "**Disadvantages**\n",
        "* The pure k-means algorithm is **not very flexible**, and as such is of limited use (except for when vector quantization as above is actually the desired use case). \n",
        "* In particular, the parameter **k is known to be hard to choose** when not given by external constraints. \n",
        "* Another limitation is that it **cannot be used with arbitrary distance** functions \n",
        "* **Cannot be used with non-numerical data.**\n",
        "\n",
        "**Quirks**\n",
        "* The number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set.\n",
        "\n",
        "### Refereces\n",
        "\n",
        "* https://developers.google.com/machine-learning/clustering\n",
        "* Kneedle : https://raghavan.usc.edu//papers/kneedle-simplex11.pdf\n",
        "* Theory : https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
        "* Theory : https://en.wikipedia.org/wiki/K-means_clustering\n",
        "* Rocchio Algorithm : https://nlp.stanford.edu/IR-book/html/htmledition/rocchio-classification-1.html\n",
        "*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Yv3GDr9IJ6"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9rfZYH89IsY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "def sklearnKMeans(train_x,k):\n",
        "  pass\n",
        "\n",
        "def KMeans(train_x,k):\n",
        "  \"\"\"\n",
        "  user defined functions to find clusters using Kmeans\n",
        "  \"\"\"\n",
        "  \n",
        "  cluster_one =np.random.normal(0,1,size=(100,2))\n",
        "  cluster_two=np.random.normal(5,1,size=(100,2))\n",
        "  \n",
        "  train_x=np.append(cluster_one,cluster_two,axis=0)\n",
        "  #print(train_x)\n",
        "  data =pd.DataFrame(train_x)\n",
        "  plt.scatter(train_x[:,0],train_x[:,1])\n",
        "  \n",
        "  k=3\n",
        "  d =data.shape[1]\n",
        "  iterations=10\n",
        "  n=len(train_x)\n",
        "  centroids=np.zeros([k,d])\n",
        "  \n",
        "  temp_centroids=np.zeros([k,d])\n",
        "  dist=np.zeros(n)\n",
        "  # Intialize a matrix for storing centoids of length k and dimensions d\n",
        "  for i in range(len(centroids)):\n",
        "    centroids[i]=random.choice([0,len(train_x)])\n",
        "    temp_centroids[i]=random.choice([0,len(train_x)])\n",
        "    \n",
        "  # Overall = O(n x k x d x i)\n",
        "  \n",
        "  for i in range(iterations):\n",
        "               \n",
        "        # Time complexity = O(n X k X d)\n",
        "        temp_centroids, dist = compute_eucledian(centroids,k,train_x,d)\n",
        "        \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "    \n",
        "KMeans(None, None)\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v76oShyYDdow"
      },
      "source": [
        "### Naive Bayes\n",
        "\n",
        "Pros:\n",
        "\n",
        "* It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
        "* When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
        "* It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
        "\n",
        "Cons:\n",
        "\n",
        "*  If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n",
        "* On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
        "* Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
        "\n",
        "\n",
        "#### Applications of Naive Bayes Algorithms\n",
        "*  time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
        "* Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\n",
        "* Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
        "* Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcaotj-Gx444"
      },
      "source": [
        "\"\"\"\n",
        "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n",
        "https://www.quora.com/What-is-the-difference-between-a-Gaussian-Multinomial-and-Bernoulli-Naive-Bayes-classifier\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# load the iris dataset \n",
        "from sklearn.datasets import load_iris \n",
        "# training the model on training set \n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "# splitting X and y into training and testing sets \n",
        "from sklearn.model_selection import train_test_split \n",
        "iris = load_iris() \n",
        "  \n",
        "# store the feature matrix (X) and response vector (y) \n",
        "X = iris.data \n",
        "y = iris.target \n",
        "  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
        "\n",
        "gnb = GaussianNB() \n",
        "gnb.fit(X_train, y_train) \n",
        "  \n",
        "# making predictions on the testing set \n",
        "y_pred = gnb.predict(X_test) \n",
        "  \n",
        "# comparing actual response values (y_test) with predicted response values (y_pred) \n",
        "from sklearn import metrics \n",
        "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5hcftQSHTyY"
      },
      "source": [
        "#Kmeans for 2 var\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "x=np.array([1,2,3,3,4,2,2,1,1])\r\n",
        "y=np.array([1,3,7,1,2,34,11,1])\r\n",
        "\r\n",
        "class Kmeans:\r\n",
        "  def __init__(self):\r\n",
        "    pass\r\n",
        "\r\n",
        "  def train(x,y,k=2):\r\n",
        "\r\n",
        "    # Assign k datapoints as random centroids\r\n",
        "    k_points=np.random.choice(len(x1),k)\r\n",
        "    c1x=x[k_points[0]]\r\n",
        "    c1y=y[k_points[0]]\r\n",
        "\r\n",
        "    c2x=x[k_points[1]]\r\n",
        "    c2y=y[k_points[1]]\r\n",
        "\r\n",
        "    # Assign rem datapoints  to the k centroids randomly\r\n",
        "    for i in range(len(x)):\r\n",
        "      if i!= k_points[0] and i!=k_points[0]:\r\n",
        "\r\n",
        "\r\n",
        "    # Iterate n times\r\n",
        "    # calculate distance between all datapoints and all centroids\r\n",
        "    # Assign datapoints to the nearest centroid\r\n",
        "    # Calculate mean centroid\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
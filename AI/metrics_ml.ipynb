{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metrics_ml.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thefr33radical/codeblue/blob/master/AI/metrics_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdlRZkclU9UU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqowXySnoLRy"
      },
      "source": [
        "### Reference Lecture\n",
        "* [metrics for ml models](https://www.youtube.com/watch?v=wpQiEHYkBys&t=322s)\n",
        "\n",
        "\n",
        "### Loss functions\n",
        "\n",
        "* Loss function represents cost associated with a event. Mathematically it maps values of 2 or more variable to a real number.\n",
        "* It shows how accurate a model is from the ground truth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AFdO_5b-dfr"
      },
      "source": [
        "## Regression Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpw4p5yE65xu"
      },
      "source": [
        "# r2 score\n",
        "'''\n",
        "Defnition : R-squared is a statistical measure of how close the data are to the fitted regression line.\n",
        "It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
        "\n",
        "Possible Values:\n",
        "\n",
        "if R2 < 0 : Horizontal line explains the data better than the model.\n",
        "if R2 = 0 : HOrizontal line explains the data equally as your model.\n",
        "if R2 > 0 : Explains variance\n",
        "\n",
        "\n",
        "  References:\n",
        "* https://stats.stackexchange.com/questions/183265/what-does-negative-r-squared-mean\n",
        "* https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emc7aDBH7i7H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKgzGM0U7lTl",
        "outputId": "b59f63fd-4030-4291-d5b1-6158c0728715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "MSE - Mean Squared Error\n",
        "* Sensitive to outliers, use when outliers are minimum.\n",
        "* Great to penalize/learn outliers.  Outliers get exponential weightage.\n",
        "* Always gives one solution, hence stable\n",
        "\n",
        "Advantage: The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on theses errors due to the squaring part of the function.\n",
        "Disadvantage: If our model makes a single very bad prediction, the squaring part of the function magnifies the error. Yet in many practical cases we don’t care much about these outliers and are aiming for more of a well-rounded model that performs good enough on the majority.\n",
        "\n",
        "References :\n",
        "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3#:~:text=Advantage%3A%20The%20MSE%20is%20great,the%20function%20magnifies%20the%20error.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "y=np.array([1,2,3,4],dtype=int)\n",
        "Yp=np.array([1,20,3,4],dtype=int)\n",
        "\n",
        "def MSE(y,Yp):\n",
        "  return sum(np.square((y-Yp)))/len(y)\n",
        "\n",
        "print(MSE(y,Yp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjB_AwKO7p9p",
        "outputId": "2f0fad6c-71d9-40ca-e2d3-21b9f9dd57ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "MAE - Mean Absolute Error\n",
        "* Insensitive to outliers, use when outliers need to be ignored.\n",
        "* All errors get equal weightage.\n",
        "* Multiple solution, hence unstable\n",
        "\n",
        "Advantage: The beauty of the MAE is that its advantage directly covers the MSE disadvantage. Since we are taking the absolute value, all of the errors will be weighted on the same linear scale. Thus, unlike the MSE, we won’t be putting too much weight on our outliers and our loss function provides a generic and even measure of how well our model is performing.\n",
        "Disadvantage: If we do in fact care about the outlier predictions of our model, then the MAE won’t be as effective. The large errors coming from the outliers end up being weighted the exact same as lower errors. This might results in our model being great most of the time, but making a few very poor predictions every so-often.\n",
        "\n",
        "References :\n",
        "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3#:~:text=Advantage%3A%20The%20MSE%20is%20great,the%20function%20magnifies%20the%20error.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "y=np.array([1,2,3,4],dtype=int)\n",
        "Yp=np.array([1,20,3,4],dtype=int)\n",
        "\n",
        "def MAE(y,Yp):\n",
        "  return sum(np.abs((y-Yp)))/len(y)\n",
        "\n",
        "print(MAE(y,Yp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5NZ8i-yAqFT",
        "outputId": "4410d5f3-9ead-409f-df65-59f397741db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "Huber Loss - Uses both MAE & MSE to balance the score.\n",
        "* use (y-Yp)^2  if |y-Yp|<=delta\n",
        "* use delta * |y-Yp| - 1/2 * delta^2 otherwise\n",
        "\n",
        "References :\n",
        "https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3#:~:text=Advantage%3A%20The%20MSE%20is%20great,the%20function%20magnifies%20the%20error.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def HuberLoss(y,Yp,delta):\n",
        "  huber_mse=np.square(y-Yp)\n",
        "  huber_mae=delta * np.abs(y-Yp) - 0.5 * delta**2\n",
        "  return sum(np.where(np.abs(y-Yp)<=delta,huber_mse,huber_mae))/len(y)\n",
        "\n",
        "y=np.array([1,2,3,4],dtype=int)\n",
        "Yp=np.array([1,20,3,4],dtype=int)\n",
        "delta=1\n",
        "y_vals=HuberLoss(y,Yp,delta)\n",
        "print(y_vals)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_YOPqql-X1-"
      },
      "source": [
        "## Classification Metrics"
      ]
    }
  ]
}